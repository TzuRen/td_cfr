\relax 
\citation{gilpin2006competitive}
\citation{gilpin2007gradient}
\citation{cfr}
\citation{mccfr}
\citation{pcs}
\citation{bayesbluff}
\citation{shortterm}
\citation{rnr}
\citation{dbr}
\citation{implicit}
\citation{cfr}
\citation{mccfr}
\citation{sutton1998reinforcement}
\citation{billings2002challenge}
\citation{cfr}
\citation{regretmatching}
\citation{pcs}
\citation{mccfr}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Counterfactual Regret Minimization}{\thepage }}
\citation{dbr}
\citation{rnr}
\citation{bayesbluff}
\citation{shortterm}
\citation{shortterm}
\citation{exp4}
\citation{implicit}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Explicit opponent modeling}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Implicit modeling}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces An illustration of how Counterfactual Regret Minimization can be cast as a dynamic programming algorithm and, consequently, transformed into a temporal difference (TD) learning algorithm. The probability distribution over the nodes before a given information set correspond to the transition function, $P$, for that information set; the value of taking a given action in a specific information set is the $Q$-value function; and the function mapping an information set to a probability distribution over actions is the policy function $\pi $.}}{\thepage }}
\newlabel{fig-tdcfr}{{1}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {3}Algorithms}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}TD-CFR}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Policy Bootstrapping}{\thepage }}
\citation{bayesbluff}
\citation{shortterm}
\citation{ponsen2009evolutionary}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces TD-CFR}}{\thepage }}
\newlabel{alg-tdcfr}{{1}{\thepage }}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces UpdatePolicy}}{\thepage }}
\newlabel{alg-tdcfr-updatepolicy}{{2}{\thepage }}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces UpdateRegret}}{\thepage }}
\newlabel{alg-tdcfr-updateregret}{{3}{\thepage }}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Policy Bootstrapping}}{\thepage }}
\newlabel{alg-policybootstrapping}{{4}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Implicit Subpolicy Modeling}{\thepage }}
\citation{bayesbluff}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Subpolicy Bootstrapping}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Setup}{\thepage }}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5}{\ignorespaces Subpolicy Discovery}}{\thepage }}
\newlabel{alg-subpolicydiscovery}{{5}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The maximum exploitability of the agents in the simple skewing population.}}{\thepage }}
\newlabel{fig-exploitability-simple}{{2}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The maximum exploitability of the agents in the complex skewing population.}}{\thepage }}
\newlabel{fig-exploitability-complex}{{3}{\thepage }}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Results}{\thepage }}
\newlabel{sec-results}{{4.2}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{\thepage }}
\citation{dbr}
\citation{dbbr}
\citation{shortterm}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The average performance of each algorithm against the population of exploitable agents derived from a simple strategy skewing methodology. Note that values near each other are not statistically significant due to overlapping confidence intervals.}}{\thepage }}
\newlabel{fig-exploitedsimple}{{4}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The average performance of each algorithm against the population of exploitable agents derived from a relatively complex strategy skewing methodology. Note that values near each other are not statistically significant due to overlapping confidence intervals.}}{\thepage }}
\newlabel{fig-exploitedcomplex}{{5}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {6}Future work}{\thepage }}
\citation{simplemodels}
\citation{simplemodels}
\bibstyle{abbrv}
\bibdata{sigproc}
\bibcite{exp4}{1}
\bibcite{implicit}{2}
\bibcite{billings2002challenge}{3}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The average performance of each algorithm against a Nash equilibrium agent.}}{\thepage }}
\newlabel{fig-exploitednash}{{6}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Results of 500 random trials calculating the maximum exploitation gain from using an implicit subpolicy approach instead of full policy implicit modeling. The vast majority of trials result in less than 0.01 bet/hand potential gain, indicating a possible reason for the failure of the implicit subpolicy modeling algorithm.}}{\thepage }}
\newlabel{fig-subpolicygain-simple}{{7}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {8}References}{\thepage }}
\bibcite{simplemodels}{4}
\bibcite{dbbr}{5}
\bibcite{gilpin2007gradient}{6}
\bibcite{gilpin2006competitive}{7}
\bibcite{regretmatching}{8}
\bibcite{pcs}{9}
\bibcite{dbr}{10}
\bibcite{rnr}{11}
\bibcite{mccfr}{12}
\bibcite{ponsen2009evolutionary}{13}
\bibcite{bayesbluff}{14}
\bibcite{shortterm}{15}
\bibcite{sutton1998reinforcement}{16}
\bibcite{cfr}{17}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Results of 500 random trials calculating the maximum exploitation gain from using an implicit subpolicy approach instead of full policy implicit modeling. The vast majority of trials result in less than 0.01 bet/hand potential gain, further supporting the hypothesis that Leduc does not benefit from subpolicy modeling.}}{\thepage }}
\newlabel{fig-subpolicygain-complex}{{8}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {A}TD-CFR Results}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The average performance of each algorithm against the population of exploitable agents derived from a simple strategy skewing methodology.}}{\thepage }}
\newlabel{fig-exploitedsimple-tdcfr}{{9}{\thepage }}
\@writefile{toc}{\contentsline {section}{\numberline {B}Confidence Intervals}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The average performance of each algorithm against the population of exploitable agents derived from a relatively complex strategy skewing methodology.}}{\thepage }}
\newlabel{fig-exploitedcomplex-tdcfr}{{10}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The average performance of each algorithm against a Nash equilibrium agent.}}{\thepage }}
\newlabel{fig-exploitednash-tdcfr}{{11}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The average performance of each algorithm against the population of exploitable agents derived from a simple strategy skewing methodology.}}{\thepage }}
\newlabel{fig-exploitedsimple-confidence}{{12}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The average performance of each algorithm against the population of exploitable agents derived from a relatively complex strategy skewing methodology.}}{\thepage }}
\newlabel{fig-exploitedcomplex-confidence}{{13}{\thepage }}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The average performance of each algorithm against a Nash equilibrium agent.}}{\thepage }}
\newlabel{fig-exploitednash-confidence}{{14}{\thepage }}
