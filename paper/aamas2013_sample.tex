\documentclass{aamas2013}

% if you are using PDF LaTex and you cannot find a way for producing
% letter, the following explicit settings may help
 
\pdfpagewidth=8.5truein
\pdfpageheight=11truein

\begin{document}

% In the original styles from ACM, you would have needed to
% add meta-info here. This is not necessary for AAMAS 2013  as
% the complete copyright information is generated by the cls-files.


\title{Short Term Opponent Exploitation in Poker}

\numberofauthors{1}

\author{
% 1st. author
\alignauthor
%Paper  XXX
Wesley Tansey\\
       \affaddr{University of Texas at Austin}\\
       \affaddr{1 Inner Campus Drive}\\
       \affaddr{Austin, TX 78712 USA}\\
       \email{tansey@cs.utexas.edu}
}

\maketitle

\begin{abstract}
Effectively exploiting opponents in incomplete information, extensive-form games is a difficult task for an online learning agent. Previous work has focused on either maintaining an explicit model that is updated directly based on observed opponent actions or implicit modeling via a portfolio of methods.

This paper introduces four new approaches to playing exploitive poker. First, a one-step temporal differencing version of counterfactual regret minimization called TD(0)-CFR is presented. Second, an alternative implicit modeling approach based on the notion of subpolicies is explored. Third, a hybrid implicit and explicit algorithm is described that leverages a portfolio of models to bootstrap an explicit model. Finally, a combination of the second and third models is discussed along with an approach automtically deriving subpolicies from a portfolio of complete policies.

The game of Leduc poker is used as a benchmark domain to compare the two standard and four novel exploitive approaches. Results from these experiments indicate that everything failed and I should just give up on life.
\end{abstract}

% Note that the category section should be completed after reference to the ACM Computing Classification Scheme available at
% http://www.acm.org/about/class/1998/.

%\category{H.4}{Information Systems Applications}{Miscellaneous}

%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%General terms should be selected from the following 16 terms: Algorithms, Management, Measurement, Documentation, Performance, Design, Economics, Reliability, Experimentation, Security, Human Factors, Standardization, Languages, Theory, Legal Aspects, Verification.

\terms{Algorithms, Experimentation}

%Keywords are your own choice of terms you would like the paper to be indexed by.

\keywords{poker, opponent modeling, online learning}

\section{Introduction}

\section{Background}

    \subsection{Counterfactual Regret Minimization}

    \subsection{OS-MCCFR}
    
    \subsection{Explicit opponent modeling}
    
    \subsection{Implicit modeling}

\section{Algorithms}

    \subsection{TD(0)-CFR}

    \subsection{Implicit Subpolicy Modeling}

    \subsection{Policy Bootstrapping}

    \subsection{Subpolicy Bootstrapping}

\section{Experiments}
Experiments were performed on all four of our proposed algorithms and the standard approaches from the literature. We next detail our experimental setup and the results for each model in each experiment.

    \subsection{Setup}
    To analyze the performance of each model, we conducted a series of experiments in the game of Leduc poker. Leduc is a two-player poker variant where the deck contains six cards (two jacks, two queens, and two kings), each player is dealt a single hole card, there two rounds of betting, and a single community card is dealt after the first round. In total, Leduc contains 144 information sets for the second player, making this game non-trivial for agent modeling while still being tractable to analye in-depth. For more information on Leduc poker, see \cite{bayesbluff}.

    Experiments were conducted against three types of stationary opponents. First, a population of 100 ``simple'' agents was generated by generating two random percentage triplets for each agent; the first triplet was then used to skew the preflop actions of a Nash equilibrium strategy and the second was used to skew the flop actions. A population of 100 ``complex'' agents was then generated in a similar manner, where with equal probability the agent's preflop or flop actions were skewed, with a similar approach used to skew actions based on the holecard of the opponent (J, Q, or K). These two populations of agents aim to capture the systematic bias presumably found in human players that often over- or under- play a given hand or play looser or tighter at different points in the game. Finally, a Nash equilibrium strategy was used as a baseline to examine how the agents perform when trying to model an optimal player.

    In each experiment, the algorithms were tested for 200 matches of 200 hands each, with each algorithm playing against each simple and complex opponent twice. The TD-CFR agent used $\epsilon=0.1$ with an exponential $\epsilon$-decay factor of $0.99$ and a learning rate of $\alpha=0.05$. For portfolio-based methods, the portfolio consisted of the Nash equilibrium strategy and four skewed strategies, chosen at random from the population at the start of each match. For explicit models, two times the Nash strategy was used as an initial prior. The subpolicy discovery algorithm used the Nash equilibrium strategy as the baseline method, $\delta_{min}^{b}=0.05$, $\delta_{max}^{sp_i}=0.1$, and a minimum subpolicy size of 3.
    \subsection{Results}


\section{Discussion and Future Work}

\section{Conclusions}

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
