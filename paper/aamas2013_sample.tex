\documentclass{aamas2013}

% if you are using PDF LaTex and you cannot find a way for producing
% letter, the following explicit settings may help
 
\pdfpagewidth=8.5truein
\pdfpageheight=11truein

\begin{document}

% In the original styles from ACM, you would have needed to
% add meta-info here. This is not necessary for AAMAS 2013  as
% the complete copyright information is generated by the cls-files.


\title{Short Term Opponent Exploitation in Poker}

\numberofauthors{1}

\author{
% 1st. author
\alignauthor
%Paper  XXX
Wesley Tansey\\
       \affaddr{University of Texas at Austin}\\
       \affaddr{1 Inner Campus Drive}\\
       \affaddr{Austin, TX 78712 USA}\\
       \email{tansey@cs.utexas.edu}
}

\maketitle

\begin{abstract}
Effectively exploiting opponents in incomplete information, extensive-form games is a difficult task for an online learning agent. Previous work has focused on either maintaining an explicit model that is updated directly based on observed opponent actions or implicit modeling via a portfolio of methods.

This paper introduces four new approaches to playing exploitive poker. First, a one-step temporal differencing version of counterfactual regret minimization called TD(0)-CFR is presented. Second, an alternative implicit modeling approach based on the notion of subpolicies is explored. Third, a hybrid implicit and explicit algorithm is described that leverages a portfolio of models to bootstrap an explicit model. Finally, a combination of the second and third models is discussed along with an approach automtically deriving subpolicies from a portfolio of complete policies.

The game of Leduc poker is used as a benchmark domain to compare the two standard and four novel exploitive approaches. Results from these experiments indicate that everything failed and I should just give up on life.
\end{abstract}

% Note that the category section should be completed after reference to the ACM Computing Classification Scheme available at
% http://www.acm.org/about/class/1998/.

%\category{H.4}{Information Systems Applications}{Miscellaneous}

%A category including the fourth, optional field follows...
%\category{D.2.8}{Software Engineering}{Metrics}[complexity measures, performance measures]

%General terms should be selected from the following 16 terms: Algorithms, Management, Measurement, Documentation, Performance, Design, Economics, Reliability, Experimentation, Security, Human Factors, Standardization, Languages, Theory, Legal Aspects, Verification.

\terms{Algorithms, Experimentation}

%Keywords are your own choice of terms you would like the paper to be indexed by.

\keywords{poker, opponent modeling, online learning}

\section{Introduction}

\section{Background}


    \subsection{Counterfactual Regret Minimization}
    The Counterfactual Regret minimization (CFR) algorithm \cite{cfr} is an iterative algorithm that extends regret matching \cite{regretmatching} to incomplete-information, extensive-form games to provably walk a strategy profile towards a correlated $\epsilon$-equilibrium.\footnote{Since Leduc poker is a two-player, zero-sum game, all correlated equilibria are also Nash equilibria.} Later work has introduced speedups to CFR in the form of game tree compression \cite{pcs} and Monte Carlo variants \cite{mccfr}. Among the MC-CFR implementations, Outcome Sampling CFR (OS-CFR) can be used for online regret minimization. The TD-CFR approach presented in this paper is based on OS-CFR.
    
    \subsection{Explicit opponent modeling}
    Explicit modeling of an opponent is perhaps the most natural way to conceive of an exploitive strategy. A probability distribution is estimated at each information set in the opponent's incomplete information gametree, with the results being updated after every observed outcome. Given the explicit model, a best response can be calculated and played. Data-Biased Responses (DBRs) \cite{dbr} take a frequentist approach to explicit modeling, leveraging a large database of perfect information hands of opponent actions to build a model of opponent play. Once the opponent model is derived, DBRs compute a Restricted Nash Response (RNR) \cite{rnr} that enables the agent to trade off exploitation and exploitability. While the resulting model is highly effective, the required large number of perfect-information hands and the sensitivity to the source of the samples makes DBRs impractical for short-term opponent exploitation.

    An alternative, Bayesian approach to explicit modeling is to maintain a dirichlet prior at every information set \cite{bayesbluff,shortterm}. After every revealed hand, priors can then be updated; after a folded hand, the priors can be updated by marginalizing out the unobserved holecards. An exact opponent strategy can then be sampled via importance sampling or taken as the \textit{maximum a posteriori} (MAP) strategy. Importance sampling is the more robust strategy since MAP inference may be a poor approximation if the distribution is multimodal. This paper uses importance sampling over a set of dirichlet priors, initialized to have a mode at the Nash equilibrium, for its explicit opponent modeling agent.
    
    \subsection{Implicit modeling}
    For large games, simple explicit modeling of an opponent may be impractical since the number of samples observed will generally cover only a small fraction of total game tree. Implicit opponent modeling \cite{shortterm} instead assumes the opponent is using a strategy drawn from some portfolio of potential strategies that is made available to the agent. After each outcome, the agent updates the likelihood of the opponent using each specific strategy, $s \in S$, given some observations, $O$, made online. Given that the parameters of $s$ are known, and assuming a fair deck that deals each hand equally likely, the likelihood of a player using a given strategy can be computed via Bayes rule: $p(s | O) = \frac{p(O | s)p(s)}{p(O)}$. As in explicit Bayesian opponent modeling, the opponent model can be chosen either as the MAP strategy or with importance sampling. Alternatively, an adversarial bandit algorithm such as Exp4 \cite{exp4} can be used to find the best strategy from the best responses to the portfolio strategies. Implicit modeling with a portfolio of 2010 ACPC competitors is currently the state-of-the-art in exploitive computer poker agents \cite{implicit}. This paper uses importance sampling over a set of five opponent models for its implicit opponent modeling agent.

\section{Algorithms}

    \subsection{TD(0)-CFR}

    \subsection{Implicit Subpolicy Modeling}

    \subsection{Policy Bootstrapping}

    \subsection{Subpolicy Bootstrapping}

\section{Experiments}
Experiments were performed on all four of our proposed algorithms and the standard approaches from the literature. We next detail our experimental setup and the results for each model in each experiment.

    \subsection{Setup}
    To analyze the performance of each model, we conducted a series of experiments in the game of Leduc poker. Leduc is a two-player poker variant where the deck contains six cards (two jacks, two queens, and two kings), each player is dealt a single hole card, there two rounds of betting, and a single community card is dealt after the first round. In total, Leduc contains 144 information sets for the second player, making this game non-trivial for agent modeling while still being tractable to analye in-depth. For more information on Leduc poker, see \cite{bayesbluff}.

    Experiments were conducted against three types of stationary opponents. First, a population of 100 ``simple'' agents was generated by generating two random percentage triplets for each agent; the first triplet was then used to skew the preflop actions of a Nash equilibrium strategy and the second was used to skew the flop actions. A population of 100 ``complex'' agents was then generated in a similar manner, where with equal probability the agent's preflop or flop actions were skewed, with a similar approach used to skew actions based on the holecard of the opponent (J, Q, or K). These two populations of agents aim to capture the systematic bias presumably found in human players that often over- or under- play a given hand or play looser or tighter at different points in the game. Finally, a Nash equilibrium strategy was used as a baseline to examine how the agents perform when trying to model an optimal player.

    In each experiment, the algorithms were tested for 200 matches of 200 hands each, with each algorithm playing against each simple and complex opponent twice. The TD-CFR agent used $\epsilon=0.1$ with an exponential $\epsilon$-decay factor of $0.99$ and a learning rate of $\alpha=0.05$. For portfolio-based methods, the portfolio consisted of the Nash equilibrium strategy and four skewed strategies, chosen at random from the population at the start of each match. For explicit models, two times the Nash strategy was used as an initial prior. The subpolicy discovery algorithm used the Nash equilibrium strategy as the baseline method, $\delta_{min}^{b}=0.05$, $\delta_{max}^{sp_i}=0.1$, and a minimum subpolicy size of 3.
    
    \subsection{Results}



\section{Discussion}

\section{Future work}
The experiments conducted in this paper reveal several opportunities for future research. In this section, we highlight some of the areas that we believe are ripe for future work:

\begin{itemize}
\item \textbf{Learning opponent models from incomplete information.} DBRs \cite{dbr} require a large number of well-chosen, perfect-information samples to form strong opponent models that can then be used online. In contrast, an approach like Deviation-Based Best Resonses (DBBRs) \cite{dbbr} can potentially model opponents through incomplete information observations, but requires a startup phase of T hands to exploit an opponent. Ideally an exploitive agent would develop models that are both immediately exploitable and feasible to gather in practice.

\item \textbf{Increasing sample efficiency.} It seems intuitive that most real-world opponents' exploitable actions are correlated in a systematic way; this intuition has been noted by other researchers \cite{shortterm} as well. Increasing the information derived from opponent observations, whether from subpolicy analysis or some other approach, is critical to the success of future exploitive agent research.

\item \textbf{Subpolicy discovery.} Uncovering which subregions of opponent policies may contain the same systematic bias holds a large amount of potential. The straight-forward approach to subpolicy discovery through better feature engineering is a pragmatic approach, but a more general approach would be more powerful and be applicable to a wider class of games.

\item \textbf{Incremental best response calculation.} When maintaining an explicit opponent model where updates are made to only a portion of the model at each point, calculating a full best response may be unnecessary. In games such as Texas Hold'em, the full best response may be too computationally expensive to compute after every hand. An approach that can update only the parts of the model that are effected at each turn could vastly speed up the algorithm, with important practical implications.

\item \textbf{Modeling non-stationary opponents.} The vast majority of exploitive play in poker has assumed a stationary opponent, likely due to the presumed drastic increase in difficulty of opponent modeling. Fortunately, while this may be the case for game theoretic non-stationary agents, humans appear to use relatively simple models that are weighted heavily towards recent observations \cite{simplemodels}. Broadening the class of opponents to include such simple agents may hit the sweet spot of analytic tractability and real-world applicability.

\item \textbf{Deceptively teaching opponents.} All exploitive poker research to date has ignored the potential for teaching opponents an exploitable model. It seems reasonable that situations would present themselves, particularly against humans or agents using ``foresight-free'' models \cite{simplemodels}, where an agent could cheaply teach an opponent a policy that is exploitable for a much larger amount.
\end{itemize}

The above list has highlighted only some of the opportunities in the area of online opponent modeling. Given the small amount of research in the area, there may be numerous additional fruitful avenues of research.

\section{Conclusions}

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{sigproc}  % sigproc.bib is the name of the Bibliography in this case
% You must have a proper ".bib" file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!
%
%APPENDICES are optional
%\balancecolumns

%\balancecolumns % GM June 2007
% That's all folks!
\end{document}
